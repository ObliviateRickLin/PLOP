{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class VGG16Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Encoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for layer in self.features:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                if layer.out_channels in [64, 128, 256, 512]:\n",
    "                    outputs.append(x)\n",
    "        return outputs\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
    "        self.dec_conv1_1 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.dec_conv1_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec_conv2_1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec_conv2_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec_conv3_1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec_conv3_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec_conv4_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dec_conv4_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        x = self.upconv1(x5)\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = F.relu(self.dec_conv1_1(x))\n",
    "        x = F.relu(self.dec_conv1_2(x))\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = F.relu(self.dec_conv2_1(x))\n",
    "        x = F.relu(self.dec_conv2_2(x))\n",
    "        \n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = F.relu(self.dec_conv3_1(x))\n",
    "        x = F.relu(self.dec_conv3_2(x))\n",
    "        \n",
    "        x = self.upconv4(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = F.relu(self.dec_conv4_1(x))\n",
    "        x = F.relu(self.dec_conv4_2(x))\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = VGG16Encoder()\n",
    "        self.decoder = UNetDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5 = self.encoder(x)\n",
    "        x = self.decoder(x1, x2, x3, x4, x5)\n",
    "        return x\n",
    "\n",
    "！需要进一步确定LSTM的内部架构\n",
    "！\n",
    "class NbrsPastLSTM(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=128, num_layers=1):\n",
    "        super(NbrsPastLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, N, sequence_length, input_size)\n",
    "        batch_size, N, seq_len, input_size = x.size()\n",
    "        x = x.view(batch_size * N, seq_len, input_size)  # Reshape for LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        # Taking the output from the last time step and reshaping back\n",
    "        out = out[:, -1, :].view(batch_size, N, -1)\n",
    "        return out\n",
    "\n",
    "class EgoPastLSTM(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=128, num_layers=1):\n",
    "        super(EgoPastLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        # Taking the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "        return out\n",
    "\n",
    "class BirdviewEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BirdviewEncoder, self).__init__()\n",
    "        \n",
    "        # Conv3D layer\n",
    "        self.conv3d = nn.Conv3d(in_channels=5, out_channels=20, kernel_size=(20, 9, 9))\n",
    "        \n",
    "        # Conv2D layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=20, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        \n",
    "        # MaxPooling layer\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # AveragePooling layer\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(6, 1))\n",
    "        \n",
    "        # Fully Connected layer\n",
    "        self.fc = nn.Linear(in_features=512, out_features=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3d(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_size, K):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, K*51)  # Assuming K possible trajectories and each trajectory has 51 dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class CompleteModel(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(CompleteModel, self).__init__()\n",
    "        \n",
    "        self.birdview_encoder = BirdviewEncoder()\n",
    "        self.ego_past_lstm = EgoPastLSTM()\n",
    "        self.nbrs_past_lstm = NbrsPastLSTM()\n",
    "        \n",
    "        # Assuming the concatenated size from the three encoders for ego car\n",
    "        ego_concat_size = 512 + 128 + 128  # 512 from Birdview, 128 from EgoPastLSTM, 128 from NbrsPastLSTM\n",
    "        \n",
    "        # Assuming the concatenated size from the two encoders for neighboring cars\n",
    "        nbrs_concat_size = 512 + 128  # 512 from Birdview, 128 from NbrsPastLSTM\n",
    "        \n",
    "        self.ego_trajectory_predictor = TrajectoryPredictor(ego_concat_size, K)\n",
    "        self.nbrs_trajectory_predictor = TrajectoryPredictor(nbrs_concat_size, K)\n",
    "\n",
    "    def forward(self, birdview, ego_past, nbrs_past):\n",
    "        birdview_encoded = self.birdview_encoder(birdview)\n",
    "        ego_past_encoded = self.ego_past_lstm(ego_past)\n",
    "        nbrs_past_encoded = self.nbrs_past_lstm(nbrs_past)\n",
    "        \n",
    "        # Concatenate the encoded outputs for ego car\n",
    "        ego_concatenated = torch.cat((birdview_encoded, ego_past_encoded, nbrs_past_encoded), dim=1)\n",
    "        \n",
    "        # Concatenate the encoded outputs for neighboring cars\n",
    "        nbrs_concatenated = torch.cat((birdview_encoded, nbrs_past_encoded), dim=1)\n",
    "        \n",
    "        ego_trajectory = self.ego_trajectory_predictor(ego_concatenated)\n",
    "        nbrs_trajectory = self.nbrs_trajectory_predictor(nbrs_concatenated)\n",
    "        \n",
    "        return ego_trajectory, nbrs_trajectory\n",
    "\n",
    "\n",
    "def nll_loss(outputs, targets, alpha=3):\n",
    "    \"\"\"\n",
    "    Negative Log Likelihood loss based on the provided equation.\n",
    "    \n",
    "    Args:\n",
    "    - outputs (torch.Tensor): The predicted trajectory distributions.\n",
    "    - targets (torch.Tensor): The ground truth trajectories.\n",
    "    - alpha (float): Weight for the y coordinate loss.\n",
    "    \n",
    "    Returns:\n",
    "    - loss (torch.Tensor): The NLL loss.\n",
    "    \"\"\"\n",
    "    # Extract the distributions for ego and neighboring cars from outputs\n",
    "    # This depends on the exact format of the outputs\n",
    "    p_ego_x, p_ego_y, p_nbrs_x, p_nbrs_y = outputs\n",
    "    \n",
    "    # Compute the log likelihoods for each point in the trajectories\n",
    "    log_likelihood_ego_x = torch.log(p_ego_x(targets[:, 0]))\n",
    "    log_likelihood_ego_y = torch.log(p_ego_y(targets[:, 1]))\n",
    "    log_likelihood_nbrs_x = torch.log(p_nbrs_x(targets[:, 2:]))\n",
    "    log_likelihood_nbrs_y = torch.log(p_nbrs_y(targets[:, 3:]))\n",
    "    \n",
    "    # Compute the NLL loss based on the provided equation\n",
    "    loss = -(log_likelihood_ego_x + alpha * log_likelihood_ego_y + log_likelihood_nbrs_x + alpha * log_likelihood_nbrs_y).sum()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in dataloader:\n",
    "            # Assuming data contains the inputs and ground truth trajectories\n",
    "            inputs, targets = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = nll_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
